{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Text Analysis with Python\n",
    "\n",
    "\n",
    "-----\n",
    "### Introduction: \n",
    "\n",
    "In this module, we wil see how to analyze text data, which is one of the most important and exciting application areas in Data Science. Text analysis is used in many applications like text categorization, text clustering, sentiment analysis, social media data analysis, spam filtering, mining text data from forms and many more. It forms the basis for natural language processing. We will discuss text classification, clustering and text mining over the next two weeks. This notebook focuses on basic text analysis tasks such as accessing data, tokenizing a corpus, and computing token frequencies. We illustrate these tasks using functionalities in **sklearn** and nltk packages in Python.\n",
    "\n",
    "\n",
    "**Reference: ** An interesting read for the introduction to [Natural Language Processing](https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Data\n",
    "\n",
    "Lets get started with our text analysis with the twenty newsgroup data set. The data can be downloaded using the inbuilt methods of scikit learn library. We will explore the data a bit before delving into text analysis.\n",
    "\n",
    "One primary use of the early Internet was to share information among interested groups via newsgroups. Users could subscribe to these groups to send and receive postings of interest. This dataset has postings to twenty newsgroups, thus the newsgroup is the classification target and the text in the posting is used to make the features. The postings are similar to emails, thus each posting will have a header, the article body, which might quote all or part of a previous message, and possibly a footer. The header, quoted text, and the footer can be removed by scikit learn by including the remove attribute, and indicating whether these sections should be removed. This attribute can take one or all of the values: header, footer, and quotes. For example, the following attribute would be used to remove both headers and footers.\n",
    "\n",
    "    `remove =('headers', 'footers')`\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.datasets.fetch_20newsgroups() is a data fetching / caching function that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files. There are train and test version of this data that you can load by supplying subset='train'/'test' option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text_data = fetch_20newsgroups(data_home='../../../../datasets/DSA-8630/newsgroups/')\n",
    "\n",
    "# To learn more about these data, use scikit learn documentation, or enter help(text_data) in an IPython code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# help(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data can be accessed via Dictionary keys\n",
    "print(text_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The real data lies in the filenames and target attributes. The target attribute is the integer index of the category. Lets print the number of records in the data at hand. Also print first target names for first 10 rows.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_data.filenames.shape)\n",
    "\n",
    "print(text_data.target.shape)\n",
    "\n",
    "text_data.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target names, i.e., the names of the twenty news groups\n",
    "\n",
    "for index, label in enumerate(text_data['target_names']):\n",
    "    print('Class {0} = {1}'.format(index, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "In above print statement {0} represents first argument in print statement. Here it is index variable, {1} refers to second argument which is label here. \n",
    "\n",
    "**Reference: ** [enumerate()](https://docs.python.org/2.3/whatsnew/section-enumerate.html)\n",
    "\n",
    "Lets display a single message and see what all data is stored. If you look at the following line of code in the below code cell,\n",
    "\n",
    "    text_data['target_names'][text_data['target'][messageID]]\n",
    "\n",
    "target is similar to target_names except that it is numeric coding of names. The second part of the code i.e., **text_data['target'][messageID]** will give the target number of the 1st message. The first part i.e., **text_data['target_names'][second part of code]** then uses this target number to print the target_name\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messageID = 1000\n",
    "\n",
    "print('Target Newsgroup: {0}'.format(text_data['target_names'][text_data['target'][messageID]]))\n",
    "print(70*'-')\n",
    "\n",
    "message = text_data['data'][messageID]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can analyze text messages by using basic Python commands. For example, we can find how many times a word appears in a text by using Python string functions. One important item to consider, however, is that, by default, Python will search for sequences of characters in a text message. Thus, if a word is also part of larger words, we will over-count the occurrences, as demonstrated in the following code cell. \n",
    "\n",
    "Observe the output of two print statements below. format(token) is inserting a space before 'to' so that it can be used to search for the word 'to'. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'to'\n",
    "\n",
    "# Create isolated words. words that are not part of any larger word. For example, to in 'distortion' is not counted as a word.\n",
    "# format(token) will result in creating a token which can be used as stanalone word for search.\n",
    "i_token = ' {0} '.format(token)\n",
    "\n",
    "print('plain word: ',token)\n",
    "print('formatted word: ',i_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Expression over count: {0}'.format(message.count(token)))\n",
    "print('Isolated Token Count: {0}'.format(message.count(i_token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The word 'to' has appeared twice - i.e. in 'distortion' and as 'to'. So when you search for number of times 'to' appeared in the text it will return 2. To avoid this, the formatted text resulted in creating a token that matched only the standalone word 'to' resulting in count 1.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The above limitation where an expression is over counted can be overcome by explicitly splitting a text into tokens. By default, in Python this is done at whitespace, but this can be changed using regular expressions which we will com  across later in this notebook. \n",
    "\n",
    "\n",
    "**Reference: ** [format()](https://docs.python.org/2/tutorial/inputoutput.html) can be used to format Input/Output operations in print statements. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "import collections as col\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message.split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**Reference: ** [counter](https://docs.python.org/2/library/collections.html#collections.Counter) \n",
    "\n",
    "A Counter is a subclass of the collections class for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. most_common() is a method in Counter class to that returns a list of the n most common elements and their counts from the most common to the least. In the previous cell we used the most_common() method to print the 40 most common words in the text accrording to their frequency.\n",
    "\n",
    "The previous cells tokenized a text document, but identical tokens with different case will be treated as distinct which is not an ideal behavior since it could undercount the occurrences of an otherwise important token. Convert text to lowercase to prevent this, by using the string lower method. The word mouse has appeared twice as [mouse, Mouse] before converting the text to lowercase. After converting it to lowercase, the total number of tokens has decreased by 1, as well as the counts of specific tokens has changed (such as mouse count is increased from 1 to 2 while Mouse is no longer a token).\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = message.lower().split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Vector Space Model\n",
    "\n",
    "\n",
    "The traditional machine learning algorithms can only operate directly on numerical data. A text document can be analyzed by generating a numerical representation by counting the number of times a word occurs (as we did with the Counter collection previously.) Another approach is to normalize the token counts by the total number of tokens, which creates a term (or token) frequency. In the below code cell, the top terms and their frequency in the message is displayed.\n",
    "\n",
    "\n",
    "**Reference: ** \n",
    "\n",
    "- Wikipedia article on [Vector Space model](https://en.wikipedia.org/wiki/Vector_space_model)\n",
    "- Wikipedia article on [Document Term Matrix](https://en.wikipedia.org/wiki/Document-term_matrix)\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below print statememt, {0:12s} means, print argument 1 with 12 spaces allocated for it. \n",
    "# You can see that for two of the results, 12 spaces is not adequate and these results are misaligned.\n",
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(20*'-')\n",
    "\n",
    "total_word_count = sum(word_count.values())\n",
    "for count in word_count.most_common(counts_to_display):\n",
    "    pp.pprint('{0:12s}: {1:4.3f}'.format(count[0], count[1]/total_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Did you ever wonder how can you classify text data made up of words when machine learning algorithms work on numerical data? The only way is to build a numerical summary of the data that the algorithms can work on. An easy approach to implement this idea is to identify all possible words in the documents and to track the number of times each word occurs. In the context of this notebook, each post is a document. This produces a (very) sparse matrix of the documents, where the columns are the possible words (or tokens) and the rows are different documents (here posts).\n",
    "\n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse matrices is more formally known as bag of words, because we effectively create the bag of words out of which the documents are constructed. In this model, each document is mapped into a vector, where the individual elements in the vector correspond to the number of times the words (associated with the particular column) appears in the document. This bag of words model transforms text into independent variables. \n",
    "\n",
    "For example, in the sentence, \"This is a great place to eat. I would recommend this place to my friends,\" the word 'this' is seen twice, the word 'place' is seen twice, and the word 'great' is seen once. There's one feature for each word in bag of words. The data has to be pre-processed like every other data. This will dramatically improve the performance of the Bag of Words method. \n",
    "\n",
    "**Reference: ** Wikipedia article on [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "With scikit learn, the CountVectorizer can be used to break the documents into tokens (in this case words), which are used to construct a bag of words for the posts. Given this tokenizer, we first need to construct the list of tokens, which we do with the fit method. Second, we need to transform our documents into this sparse matrix, which we do with the transform method. Since both steps use the same input data, there is a convenience method to perform both operations at the same time, called fit_transform.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: **\n",
    "\n",
    "- [CountVectorizer documenttaion](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [CountVectorizer example](http://adataanalyst.com/scikit-learn/countvectorizer-sklearn-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the `CountVectorizer` we can see the number of words in our _bag_ as well as the number of documents on which we train, which in this case agrees with the values we obtained when we read in the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary from our data\n",
    "cv.fit(text_data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: **\n",
    "\n",
    "- [Working with text - creating Document Term Matrix](https://de.dariah.eu/tatom/working_with_text.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We need an iteratable to apply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transforming a single message is easier to comprehend. By default, scikit learn uses sparse matrices for text processing\n",
    "# It returns a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "# In sparse format number of tokens indicate size of dataset vocabulary. \n",
    "# So there is 1 document and 130107 featues in the dtm.\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# You can't explore the document-term matrix when it is in sparse form. We can convert from sparse to dense form to explore \n",
    "# the document-term matrix. The range given below is chosen randomly. \n",
    "# Each word is a feature. Below zeros indicate the words/features in columns 1000 to 1100, those words do not appear in \n",
    "# the input message. Thats why we have zeros for those cells\n",
    "print(dtm.todense()[:,1000:1100])\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements. \n",
    "print('Cells from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(80*'-')\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Find non-zero elements. scipy.sparse.find() returns the indices and values of the nonzero elements of a matrix.\n",
    "# i,j contains the row and column indices where non zero matrix entries are present while V has the entry's value.\n",
    "i, j, V = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, V))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Look at the documentation for [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for what it returns. One of the things it returns is vocabulary_ which is a dictionary. Its a mapping of terms to feature indices/columns in the Document Term Matrix (dtm). So if you look at the below cell it will return the column number in dtm where confuse exists as a column/feature in the dtm. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_[\"confuse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**itemgetter()** returns an object that in turn fetches an item from its operand. If multiple items are specified, it returns a tuple of lookup values. Look at the documentation for [itemgetter()](https://docs.python.org/2/library/operator.html#). Search for itemgetter in the page. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "print(itemgetter(1,3,5)('ABCDEFG'))\n",
    "\n",
    "\n",
    "# Lets look at another example on how itemgetter() works. \n",
    "# inventory is a list with 4 tuples\n",
    "inventory = [('apple', 3), ('banana', 2), ('pear', 5), ('orange', 1)]\n",
    "\n",
    "# getcount has an object that will fetch items at index 1 on what ever it is called upon.\n",
    "getcount = itemgetter(1)\n",
    "\n",
    "# Since key=getcount, get count will fetch the values 3,2,1,4 which are present at index 1 in inventory.\n",
    "# Based on this key the items in inventory are sorted\n",
    "sorted(inventory, key=getcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can grab the words in our _bag of words_ by extracting the _vocabulary_. This allows us to see if particular words are present in the documents. We have created a list dtm_list with the frequencies of words that appeared in the dtm and their row column indices. The below code cell prints the first 5 elemnets in the list. \n",
    "\n",
    "We can also find which term occurs most frequently, least frequently, as well as the overall top terms in following cells. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dtm_list, key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the terms in the vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look for a single term confuse\n",
    "search_word = 'confuse'\n",
    "print(\"Chosen Word ({0}): Column = {1}\".format(search_word, terms[search_word]))\n",
    "\n",
    "# Find the maximum value in dtm_list in 3rd column which will be 114455\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# Find the minimum value in dtm_list in 3rd column which will be 2336\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# In the below two lines, terms.keys() will return all keys - i.e. the column names(words).\n",
    "# For loop iterates over all this words to see get the column name which matches the column index we have in max_key and min_key\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "# the for loop above returned a list as output. So x_max is a list with a column name as same with x_min\n",
    "print(\"Max Word ({0}): Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word ({0}): Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK\n",
    "\n",
    "The scikit learn library can help us do general purpose basic text analysis. But text analysis in itself is an extremely large and growing topic. To handle advanced text analysis, we will use a library called Natural Language ToolKit or [NLTK](http://www.nltk.org). It enables a wide range of text analyses either on its own, or in conjunction with scikit learn \n",
    "\n",
    "Refer to the [NLTK documentation](http://www.nltk.org/book/ch01.html). Focus on sections 2, 3, 5, and 6 from Chapter 1. These sections cover most of the topics that are demonstrated above. We will explore how to use NLTK to perform basic text analysis in next few cells. Import the library and tokenize the message. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell Run this cell once to download the nltk datasets. Once the data is downloaded you can comment out \n",
    "# this cell or delete it. \n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "top_display=10\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**nltk.FreqDist() :**  This function gives us the frequency of each vocabulary item in the text. It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items.\n",
    "\n",
    "\n",
    "**Regular Expressions: ** We can clean up the list of tokens by using a regular expression with the `word_tokenize` method. One benefit of using regular expressions is that we can specifically indicate of what a token should be composed. In this case, we state a token is a sequence of one or more alphanumeric characters surrounded by white space. The regular expression ^\\w\\s identifies tokens as one or more alphanumeric characters followed by a whitespace character. Doing this removes the punctuation tokens, as shown below. You can compare the output of previous code cell and next cell. The symbols (':', '@', '.', ',', '(') are no more counted as tokens.\n",
    "\n",
    "^\\w\\s\n",
    "\n",
    "- ^ - asserts position at start of the string\n",
    "\n",
    "- \\w - matches any word character (equal to [a-zA-Z0-9_])\n",
    "\n",
    "- \\s - matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Regular Expression to parse a text document\n",
    "import re\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message))]\n",
    "\n",
    "# Count token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "You can measure the diversity of terms which is the fraction of unique tokens, or terms, in a document to the total tokens or terms in a document. See: [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity). \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can compute the number of [unique sample values](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.B), [number of samples outcomes](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N), and the [maximum occurring token](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N) with simple NLTK statistical functions. We can also iterate through and display the most commonly occurring terms and their counts.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print('{0:12s}:  {1:4.3f}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Words that occur rarely are important sometimes. For example, in a classification process, words that are uniquely assigned to a particular message should carry more weight. Words that only occur once in an entire set of documents, or corpus, provide unique insight into the particular text document in which they occur. A word that only occurs once in an entire corpus is known as a [_hapax_](https://en.wikipedia.org/wiki/Hapax_legomenon). NLTK has a `hapaxes` method that can be used to quickly find _hapaxes_ in a corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "NLTK can be used to see the most commonly used tokens. `tabulate` method can display the top tokens and their frequency. Once the counts are displayed, we will visually plot the counts of the top tokens by using the `plot` method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elements to display\n",
    "top_display=10\n",
    "counts.tabulate(top_display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "sns.despine(offset=5)#, trim=True)\n",
    "counts.plot(top_display, cumulative=False)\n",
    "axs.set_title('Term Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cumulative=True in above plot and re run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## NLTK Corpus\n",
    "\n",
    "The NLTK library includes a number of [data sets](http://www.nltk.org/nltk_data/) that can be downloaded and used directly from within NLTK. Let's use the NLTK [movie review corpus](http://www.cs.cornell.edu/people/pabo/movie-review-data/). \n",
    "\n",
    "In the following code cells, we access the movie review data set, display (part) of the data set's README (or general documentation), before we begin to process the words or terms in the corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access the NLTK movie review data set. \n",
    "\n",
    "mvr = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data set README\n",
    "print(mvr.readme()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvr_words = mvr.words()\n",
    "counts  = nltk.FreqDist(mvr_words)\n",
    "num_words = len(mvr_words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Movie Review has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\".format(num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(mvr.words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The data are organized into separate files for each movie review. Since these reviews have an associated sentiment, negative and positive, the reviews are categorized (via a directory structure) into `neg` or `pos` respectively. We can directly access a single review, which can be treated as a single text document. In the next few code cells we directly access the number of files, which can be used to count the number of reviews (assuming one review per file). We also display the contents of a single file, before displaying a subset of the files in one particular category, in this case `neg`, or negative reviews.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each article is in a separate file\n",
    "\n",
    "print('Total Number of reviews = {0}'.format(len(mvr.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filename = mvr.fileids()[0]\n",
    "print('Example File: {0}'.format(a_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display first 199 characters of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print part of the file\n",
    "pp.pprint(mvr.raw(a_filename)[:199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display article assigned categories\n",
    "pp.pprint(mvr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first 20 articles that have negative category\n",
    "pp.pprint(mvr.fileids('neg')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the contents of a file, we can process the associated text in the same manner as before. In this case, we tokenize one review into sentences as opposed to the traditional word tokens. The sents() function divides the text up into its sentences, where each sentence is a list of words:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentences from an article\n",
    "\n",
    "a_filename = 'neg/cv779_18989.txt'\n",
    "for sent in mvr.sents(a_filename):\n",
    "    pp.pprint(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are listing out words that are much longer than normal. As this simple example demonstrates, this can be a useful technique to search for potential problems, since in this case, none of the example shown are actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can process the words with normal Python\n",
    "# For example, print out really long words\n",
    "long_words = [word for word in mvr_words if len(word) > 22]\n",
    "long_words.sort(reverse=True)\n",
    "pp.pprint(long_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
