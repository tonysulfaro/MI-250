{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Text Classification\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Text documents often contain many occurrences of the same word. For example, in a document written in _English_, words such as _a_, _the_, _of_, and _it_ likely occur very frequently. When classifying a document based on the number of times specific words occur in the text document, these words can lead to biases, especially since they are generally common in **all** text documents you might want to classify. As a result, the concept of [_stop words_](https://en.wikipedia.org/wiki/Stop_words) was invented. Basically these words are the most commonly occurring words that should be removed during the tokenization process in order to improve subsequent classification efforts. \n",
    "\n",
    "We can easily specify that the __English__ stop words should be excluded during tokenization by using the `stop_words`. Note, _stop word_ dictionaries for other languages, or even specific domains, exist and can be used instead. We demonstrate the removal of stop words by using a `CountVectorizer` in the following simple example.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)\n",
    "\n",
    "# Sample sentance to tokenize\n",
    "my_text = 'This module introduced many concepts in text analysis.'\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "pp.pprint(tk_func1(my_text))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Tokenization (with Stop words):')\n",
    "pp.pprint(tk_func2(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "So far, we have looked at several techniques to remove redundant or unimportant features. For example, we changed the case of all text to lowercase and we have applied stop words. However, there still is the issue of different forms of the same word, for example compute, computer, computed, and computing. The process of changing words back to their root, or basic form (by removing prefixes and suffixes) so that token frequencies match the use of the root token rather than being spread across multiple similar tokens is known as [stemming](https://en.wikipedia.org/wiki/Stemming). \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming, is the _Porter Stemmer_, which was originally published in 1980 by Martin Porter. An improved version was released in 2000, which fixed a number of errors. NLTK includes the Porter Stemmer, which can be used with scikit learn by creating a special function that tokenizes text documents and passing this function as an argument to the `CountVectorizer` via the `tokenizer` attribute. By performing stemming inside this tokenize method, we can return a set of tokens for a document that have been stemmed. In the following code cell, we use a custom `tokenize` method that first builds a list of tokens by using nltk, and then maps the Porter stemmer to the list of tokens to generate a stemmed list.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to be very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Classification\n",
    "\n",
    "We identified the features (or tokens in the training documents) that we can use to classify the documents. Before we introduce a  classification technique on the newsgroups data, be aware that many issues might affect a classification process. In the context of this notebook, the data we have is similar to emails. Exclude email address information (like com, edu, etc.), proper names, and information such as dates, monetary information etc. The content in some categories will clearly overlap, such as _alt.atheism_ and _soc.religion.christian_. \n",
    "\n",
    "Issues like this demonstrate the **need** for manual intervention and introspection during the machine learning process. You would want to continually analyze classification results to ensure you understand what is occurring and why it is occurring.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "One of the simplest techniques for perfomring text classification is the [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). Fundamentally this method applies Bayes theorem by (naively) assuming independence between the features. In scikit learn, we will use a [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) model, where we treat each feature independently. Thus we calculate the likelihood of a feature corresponding to each training label, and the accumulation of these likelihoods provides our overall classification. By working with log-likelihoods, this accumulation becomes a simple sum.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='../../../../datasets/DSA-8630/newsgroups/', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='../../../../datasets/DSA-8630/newsgroups/', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['target'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the above code but is implemented using the pipeline function in sklearn. [Pipelines](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html) allows you to chain transformers and estimators together in such a way that you can use them as a single unit. Here vectorizer => classifier is made easier to work with using the Pipeline class. The fit() method of CountVectorizer() below will learn the vocabulary dictionary of all tokens in the input data train['data']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "clf = clf.fit(train['data'], train['target'])\n",
    "predicted = clf.predict(test['data'])\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['data'], test['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IFD\n",
    "\n",
    "Previously, we have simply used the number of times a token (i.e., word, or more generally an n-gram) occurs in a document to classify the document. Even with the removal of stop words, however, this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). An alternative technique that often provides robust improvements in classification accuracy is to employ the frequency of token occurrence, normalized over the frequency with which the token occurs in all documents. In this manner, we give higher weight in the classification process to tokens that are more strongly tied to a particular label. \n",
    "\n",
    "Formally this concept is known as [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf–idf) (or tf-idf), and scikit-learn provides this functionality via the [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) that can either follow a tokenizer, such as `CountVectorizer` or can be combined together into a single transformer via the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tools = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "# set_params() of TfidfVectorizer below, sets the parameters of the estimator. The method works on simple estimators as \n",
    "# well as on nested objects (such as pipelines). The pipelines have parameters of the form <component>__<parameter> \n",
    "# so that it’s possible to update each component of a nested object.\n",
    "clf.set_params(tf__stop_words = 'english')\n",
    "\n",
    "clf = clf.fit(train['data'], train['target'])\n",
    "predicted = clf.predict(test['data'])\n",
    "\n",
    "print(\"NB (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['data'], test['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) is typically employed on categorical variables, such as yes/no decision, or win/loss likelihoods. In the case of many labels, we can use the trick that logistic regression can quantify the likelihood a vector is in or out of a particular category. Thus, by computing this over all categories we can determine the best label for each test vector. [scikit_learn](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) provides an implementation that can be easily used for our classification problem.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(stop_words = 'english')),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "clf = clf.fit(train['data'], train['target'])\n",
    "predicted = clf.predict(test['data'])\n",
    "\n",
    "print(\"LR (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['data'], test['target'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
