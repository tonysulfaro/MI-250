{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Linguistics: Parts of Speech\n",
    "\n",
    "First, parts of speech is a linguistic concept. In english, for example, [linguists identify 8 parts of speech](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html). For computational lingustics, parts of speech are used for syntactic parsing. If you have a rule based grammar where your symbols are POS tags, then you could generate the tree representation of the sentence. Or, if you have a statistical tree parser, then it can use the POS tags as features for determining the tree/parse structure. What does this mean? Language follows structure, and you can use that structure as a way to feed a computer program that helps you understand what's happening in a large corpora of text without reading the whole thing. They are very useful for identifying features in machine learning. For example, one feature of online discussion forums is the \"dyadic interaction\". In my own research, I ignore discussion forum posts that have no response. They are \"syntactically isolated\". Computational Linguistics is also useful for taking on 'ontology' and running it over a corpora.  For example, I am currently looking at notes taken by nurses in a health system and identifying their use of \"care coordination keywords\", which I refer to as an Ontology (as do others). \n",
    "\n",
    "Lets take a look, below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "## Libraries you need\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 1. Load a (training) corpus.\n",
    "# In the code below, the corpus will be\n",
    "# referred to by variable all_text\n",
    "corpus = PlaintextCorpusReader('.','baum-oo.txt')\n",
    "all_text = nltk.Text(corpus.words())\n",
    "\n",
    "# make the training text lowercase\n",
    "all_text_lower = [x.lower() for x in all_text]\n",
    "freq_dist = FreqDist(all_text_lower)\n",
    "\n",
    "# make a reduced vocabulary\n",
    "# In choosing a vocabulary size there is a trade-off.\n",
    "# A larger vocabulary will in principle make for a more accurate\n",
    "# tagger, but will be slower and will have a greater risk of underflow.\n",
    "\n",
    "## The vocabulary is essentially the limited list of more common words that I will focus on. \n",
    "vocab_size = 500\n",
    "vocab = sorted(freq_dist.keys(), key=lambda x : freq_dist[x], reverse=True)[:vocab_size]\n",
    "# \"***\" is for all out-of-vocabulary types\n",
    "vocab = vocab + [\"***\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagsets: Penntb\n",
    "\n",
    "One of the ways parts of speech are organized is using a tagset.  The definitions of words that fall into each tagset are embedded in the NLTK library. Below, you will notice abbreviations. [Those abbreviations are defined in the tagset, which I have copied for you here.](./tagset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Make a reduced form of the PennTB tagset\n",
    "penntb_to_reduced = {}\n",
    "# noun-like\n",
    "for x in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'EX', 'WP'] :\n",
    "    penntb_to_reduced[x] = 'N'\n",
    "# verb-like\n",
    "for x in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD', 'TO'] :\n",
    "    penntb_to_reduced[x] = 'V'\n",
    "# adjective-like\n",
    "for x in ['POS', 'PRP$', 'WP$', 'JJ', 'JJR', 'JJS', 'DT', 'CD', 'PDT', 'WDT', 'LS']:\n",
    "    penntb_to_reduced[x] = 'AJ'\n",
    "# adverb-like\n",
    "for x in ['RB', 'RBR', 'RBS', 'WRB', 'RP', 'IN', 'CC']:\n",
    "    penntb_to_reduced[x] = 'AV'\n",
    "# interjections\n",
    "for x in ['FW', 'UH'] :\n",
    "    penntb_to_reduced[x] = 'I'\n",
    "# symbols\n",
    "for x in ['SYM', '$', '#'] :\n",
    "    penntb_to_reduced[x] = 'S'\n",
    "# groupings\n",
    "for x in ['\\'\\'', '(', ')', ',', ':', '``'] :\n",
    "    penntb_to_reduced[x] = 'G'\n",
    "# end-of-sentence symbols\n",
    "penntb_to_reduced['.'] = 'E'\n",
    "\n",
    "reduced_tags = ['N', 'V', 'AJ', 'AV', 'I', 'S', 'G', 'E']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label each word in the corpus according to its \"part of speech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. tag the corpus\n",
    "## nltk.pos_tag does ... \"natural language tool kit\" \"part of speech\" \"tagging\" \n",
    "## \"tagging\", you may no, is a fancy computer science word for \"labelling\" or \"classifying\"\n",
    "all_tagged = nltk.pos_tag(all_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. make the probability matrices\n",
    "# a tally from types to tags; a tally from tags to next tags\n",
    "tag_word_tally = {y:{x:1 for x in vocab} for y in reduced_tags}\n",
    "tag_transition_tally = {y:{x:1 for x in reduced_tags} for y in reduced_tags}\n",
    "\n",
    "\n",
    "previous_tag = 'E' # \"ending\" will be the dummy initial tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging and Counting the Whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (word, tag) in all_tagged :\n",
    "    # fill this out:\n",
    "        # For most tags, you want to convert it to the reduced tag\n",
    "        # For most tags and words, update the tag transition tally\n",
    "        # and the word-tag tally\n",
    "        # But what if the tag is '-NONE-'?\n",
    "        # What if the word is not in the vocabulary?\n",
    "    word = word.lower()\n",
    "    #exception handlers\n",
    "    #if tag is -NONE-, NLTK exception, jump over the word\n",
    "    if tag == '-NONE-':\n",
    "        print(\"(%s, %s) cannot be tagged\", word, tag)\n",
    "        continue \n",
    "    #if word is not in vocab\n",
    "    if word not in vocab:\n",
    "        word = '***'\n",
    "    reduced_tag = penntb_to_reduced[tag]\n",
    "    tag_word_tally[reduced_tag][word] += 1\n",
    "    tag_transition_tally[previous_tag][reduced_tag] += 1\n",
    "    previous_tag = reduced_tag\n",
    "\n",
    "# now, make the actual transition probability matrices \n",
    "trans_probs = {}\n",
    "for tg1 in reduced_tags :\n",
    "    # fill this out:\n",
    "        # For each tag tg1 compute the probabilities for transitioning to\n",
    "        # each tag (say, tg2). Using relative frequency estimation,\n",
    "        # that would mean dividing the number of times tg2 follows tg1 by\n",
    "        # the absolute number of times t1 occurs. (But, what if tg1 never occurs..?)\n",
    "        # Recommendation: think in terms of \"for each tg2, how many times had\n",
    "        # we transitioned from tg1?\"\n",
    "    count = []\n",
    "    for tg2 in reduced_tags:\n",
    "        count.append(tag_transition_tally[tg1][tg2])\n",
    "    sum_tg1 = sum(count)\n",
    "    trans_probs[tg1] = {}\n",
    "    for tg2 in reduced_tags:\n",
    "        trans_probs[tg1][tg2] = tag_transition_tally[tg1][tg2]/sum_tg1\n",
    "\n",
    "# similarly for the emission (observation) probabilities\n",
    "\n",
    "emit_probs = {}\n",
    "for tg in reduced_tags:\n",
    "    count = []\n",
    "    for word in vocab:\n",
    "        count.append(tag_word_tally[tg][word])\n",
    "    sum_w = sum(count)\n",
    "    emit_probs[tg] = {}\n",
    "    for word in vocab:\n",
    "        emit_probs[tg][word] = tag_word_tally[tg][word] / sum_w\n",
    "    # fill this out:\n",
    "    #     For each tag tg1 compute the probabilities for emitting each word v.\n",
    "    #     Recommendation: think in terms of \"for each word v, how many times\n",
    "    #     did tg1 emit v?\"\n",
    "\n",
    "\n",
    "def test_case(tokens, tgs):\n",
    "    count = 0\n",
    "    mis_classified = []\n",
    "    n_tgs = nltk.pos_tag(tokens)\n",
    "    for i in range(1,len(tgs)):\n",
    "        word, tg = n_tgs[i-1]\n",
    "        n_word, n_tg = tgs[i]\n",
    "        if n_tg == penntb_to_reduced[tg]:\n",
    "            count += 1\n",
    "        else:\n",
    "            mis_classified.append(tgs[i])\n",
    "    return count/len(tgs), mis_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect some of the variables you created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehension Check\n",
    "\n",
    "What do you notice about the symbols in the trans_probs variable? \n",
    "\n",
    "\n",
    "Where do they come from? \n",
    "\n",
    "\n",
    "\n",
    "What do you think the heirarchy means? How would you interpret it? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Viterbi\" Algorithm\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.\n",
    "\n",
    "It is now also commonly used in speech recognition, speech synthesis, diarization, keyword spotting, computational linguistics, and bioinformatics. For example, in speech-to-text (speech recognition), the acoustic signal is treated as the observed sequence of events, and a string of text is considered to be the \"hidden cause\" of the acoustic signal. The Viterbi algorithm finds the most likely string of text given the acoustic signal.\n",
    "\n",
    "[From Wikipedia:](https://en.wikipedia.org/wiki/Viterbi_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. implement Viterbi. \n",
    "# Write a function that takes a sequence of tokens,\n",
    "# a matrix of transition probs, a matrix of emit probs,\n",
    "# a vocabulary, a set of tags, and the starting tag\n",
    "\n",
    "def pos_tagging(sequence, trans_probs, emit_probs, vocab, tags, start) :\n",
    "    token = sequence[0]\n",
    "    tagged = [start]\n",
    "    mat = [0] * (len(sequence)+1)\n",
    "    mat[0] = 1\n",
    "    previous_tag = start\n",
    "    for i in range(0, len(sequence)):\n",
    "        probs = []\n",
    "        word = sequence[i]\n",
    "        if word not in vocab:\n",
    "            word = '***'\n",
    "        for tg in reduced_tags:\n",
    "            probs.append((tg, mat[i] * trans_probs[previous_tag][tg]* emit_probs[tg][word.lower()]))\n",
    "        sorted_tgs = sorted(probs, key = lambda tup: tup[1], reverse = True)\n",
    "        n_tg, max_prob = sorted_tgs[0]\n",
    "        # if n_tg == 'E':\n",
    "        #     n_tg, _ = sorted_tgs[1]\n",
    "        # argmax S_ P(O_ | S_, lambda)\n",
    "        tagged.append((sequence[i], n_tg))\n",
    "        mat[i+1] = max_prob\n",
    "        previous_tag = n_tg\n",
    "    return tagged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. try it out: run the algorithm on the test data\n",
    "tests = []\n",
    "tests.append(\"Kaliko ran out as fast as his spindle legs could carry his fat, round body, and soon the Chief Counselor entered the cavern.\")\n",
    "tests.append(\"They did not tell their niece the sad news for several days, not wishing to make her unhappy; but one morning the little girl found Aunt Em softly crying while Uncle Henry tried to comfort her.\")\n",
    "tests.append(\"Dorothy laughed merrily at this speech, and then she became very sober again, for she could see how all this trouble was worrying her aunt and uncle, and knew that unless she found a way to help them their future lives would be quite miserable and unhappy.\")\n",
    "tests.append(\"Guph went to the King's private cave and sat down upon an amethyst chair and put his feet on the arm of the King's ruby throne.\")\n",
    "\n",
    "for test in tests:\n",
    "    test_sample = nltk.word_tokenize(test)\n",
    "    test_tagged = pos_tagging(test_sample, trans_probs, emit_probs, vocab, reduced_tags, 'E')            \n",
    "    print(\"test_tagged\")\n",
    "    per, mis_classified = test_case(test_sample, test_tagged)\n",
    "    print(\"\")\n",
    "    print( \"Accuracy: (%)\",round(per * 100, 2) )\n",
    "    print( \"Missclassified: \", mis_classified )\n",
    "    print( \"==============\" )\n",
    "    print( \"\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
